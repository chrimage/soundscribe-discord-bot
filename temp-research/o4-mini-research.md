Great, I’ll dive into researching all viable self-hosted solutions for recording high-fidelity, multi-user Discord audio suitable for transcription and speaker diarization. This will include official API methods, open-source libraries, and architectural best practices, along with any Discord policy limitations.

I’ll update you once I have a comprehensive comparative analysis ready.


# Recording High-Fidelity Multi-User Discord Audio for Transcription and Diarization

## Discord Voice API: Separate Audio Streams and Metadata

**Per-User Audio Streams** – The Discord Voice API sends each participant’s audio as a separate stream (not a single mixed track). When a bot joins a voice channel and listens, it receives individual Opus-encoded audio packets tagged by a unique source ID (SSRC) for each speaking user. In practice, libraries like discord.js allow you to **create a stream for each user** in the channel; there is no built-in way to get one pre-mixed stream of all users. This per-user separation is crucial for speaker diarization, as it lets you capture each speaker’s voice independently.

**Speaking Events (Start/Stop)** – Alongside audio data, Discord provides real-time metadata about who is speaking. The voice gateway emits a **“Speaking” event** whenever a user starts or stops talking, including that user’s ID and SSRC. In fact, at least one speaking-start event is guaranteed to arrive **before** any audio frames, so the receiver can map the SSRC to the correct user ID. Most bot libraries surface this as an event or flag – for example, Discord’s voice receive extensions let you query if a member is currently speaking (the same indicator as the green circle in the UI) and handle speaking state changes via callbacks. These events enable you to detect when a user begins or ends an utterance, which is useful for segmenting audio or triggering transcription.

## Connecting to Voice and Receiving Audio

**Voice Gateway and Audio Data** – To record audio, your bot must connect to the Discord Voice Gateway (a separate WebSocket endpoint) by joining a voice channel. Officially, you do this by calling the appropriate library method (e.g. `joinVoiceChannel` in discord.js or `VoiceChannel.connect()` in discord.py), which under the hood negotiates voice server details and encryption keys. Once connected, the bot begins receiving RTP packets of Opus audio from the voice UDP socket. Discord’s voice protocol uses 48 kHz Opus codec (stereo by default), with all audio encrypted during transit. There is **no high-level “record” API** from Discord – bots must directly capture and decode these Opus packets. In fact, voice receiving for bots isn’t officially documented by Discord and is based on reverse-engineering; it’s considered a “second class” feature. Nonetheless, it is possible and widely done via established libraries and custom code.

**Libraries/Methods for Voice Receive** – Several open-source libraries provide ready-made solutions to receive and record Discord audio:

* **Node.js (discord.js)** – The official Discord.js voice module (`@discordjs/voice`) supports audio receive. You can subscribe to a user’s audio stream via the voice `receiver`. For example, using `connection.receiver.subscribe(userId, { end: AfterSilence })` will give you a readable stream of that user’s Opus audio, ending when they are silent. Discord.js creates one such stream per user, which you can decode and save. (By design it doesn’t mix them for you – if you want a combined track, you must mix the audio yourself, which is non-trivial due to synchronization issues).
* **Python (discord.py with extensions)** – The base `discord.py` library did not natively support voice receive, but an **official extension** called `discord-ext-voice-recv` is now available. This extension provides a `VoiceRecvClient` and uses an `AudioSink` paradigm to capture incoming audio in a similar way to how discord.py handles outgoing audio. Using this, you can listen in a voice channel and receive PCM audio per user. (Older community projects like `discord-ext-audiorec` also attempted this, but `discord-ext-voice-recv` is the more up-to-date solution, released in 2025.)
* **Other Libraries** – In other languages, there are equivalents. For example, **Discord.NET (C#)** and **JDA (Java)** both allow audio capture. JDA provides an `AudioReceiveHandler` with options to get either combined audio or per-user audio frames (via `UserAudio` objects). Similarly, libraries like **D++ (C++)** and **eris (Node)** have voice receive capabilities. These are mature solutions if your stack is in those languages.
* **Existing Bots (for reference)** – The best-known multi-user recording bot is **Craig**. Craig is open-source and can record a voice channel with **each speaker on a separate track**. While you intend to build your own service, Craig’s approach validates that multi-track recording *is* achievable with Discord’s API. (Craig uses multiple processes/shards to handle large recordings and manages to keep all tracks synchronized).

**Discord API Constraints** – Note that your bot must be **self-deafened** (deafening the bot’s audio output) if it’s only recording, to avoid echo or feedback. Also ensure you enable the `GUILD_VOICE_STATES` intent in your bot settings; without it, the bot cannot properly join or stay in voice. Only one voice connection per guild (server) is allowed per bot, so if you need to record multiple channels simultaneously, you’d need multiple bot instances or accounts (which gets complicated). Typically one bot can only be in one voice channel at a time.

## Compliance with Discord’s Terms (Recording Consent)

Before implementing any recording, it’s critical to address Discord’s **Terms of Service and policies**. Discord **prohibits recording voice conversations without the consent of all participants**. According to Discord support, recording calls without everyone’s prior consent *violates Discord’s TOS* and may even be illegal under privacy laws. In other words, you **must inform and get approval** from users before recording them. Many bots handle this by posting a warning message when they start recording, or requiring an explicit command from a user in the channel (which implicitly alerts everyone that a recording bot has joined).

Even with consent, Discord’s developer policy is cautious about voice-recording bots. There is no official endorsement for real-time transcription or recording features in the API. In fact, Discord’s documentation and staff have indicated they **“don’t officially support”** voice recording bots. This doesn’t mean you can’t build one – only that you’re doing so at your own risk of breaking if Discord changes how voice works. To stay within the rules, be transparent with users, and ideally offer an opt-in mechanism. It’s good practice to have your bot **announce its presence and purpose** (e.g., “This channel is being recorded for transcription”) to fulfill consent requirements.

## Architecture: From Audio Capture to Transcription

**Real-Time Streaming vs Post-Processing** – There are two broad approaches for transcribing and analyzing Discord audio:

1. **Real-Time Transcription Pipeline**: As the bot receives audio, it streams it into a speech-to-text system on the fly. This is typically done by feeding audio frames or short chunks into a streaming ASR (Automatic Speech Recognition) API or library. For example, one could pipe the audio to OpenAI’s Whisper or an API like Google Cloud Speech in real time. A practical pattern is to use Discord’s speaking events or silence detection to segment utterances, then send those segments for transcription. Using discord.js, you might subscribe to a user’s audio with an **“end after 1s of silence”** setting, so you get a stream that auto-terminates when the person finishes speaking. You can then take that audio chunk (e.g. a few seconds of speech) and run it through a transcription model. In real-time mode, you’d accumulate partial results and perhaps even send live transcripts back to a channel or use them to generate immediate responses. The AssemblyAI example demonstrates this: it opens a websocket to a live transcription service, sends audio packets as they come in, and receives incremental transcripts (with a final transcript delivered when the user stops talking).
   *Advantages:* Users get near-instant feedback or interactive experiences (e.g. a voice assistant that answers questions).
   *Challenges:* Real-time transcription is resource-intensive. Whisper, for instance, is not truly streaming – you might simulate streaming by transcribing in overlapping chunks of a few seconds, but that introduces complexity and latency. For genuine low-latency needs, you may use a faster but slightly less accurate model or a dedicated streaming API. Additionally, synchronizing multiple speakers in real-time is complex – you might need a separate transcription stream per speaker to keep their audio isolated.
2. **Post-Processing (Record-Then-Transcribe)**: The bot records the entire session (or sizable chunks of it), saves the audio to storage, and only after the session (or at intervals) sends it for transcription. For example, the bot could write each user’s audio to a file (`channelID_userID_timestamp.pcm` or `.wav` as suggested) continuously during the call. Once the call ends (or every N minutes), these files are processed by an offline speech recognition engine like Whisper, or uploaded to a transcription service that accepts whole audio files (such as AssemblyAI’s batch transcription endpoint, Google Cloud Speech batch mode, etc.). You could even convert the per-user audio into a multi-track format (e.g. an Audacity project or an MP4 with multiple audio streams) if needed.
   *Advantages:* Maximizes transcription accuracy by allowing use of powerful offline models on the full-quality audio. No worry about real-time performance – you can take your time transcribing after recording. Also, having the full recording allows additional processing like speaker diarization, corrections, or auditing the transcription.
   *Drawbacks:* It’s not live – you only get the transcript after the fact. For a summarization service, this might be fine. Another consideration is storage: recording many hours of audio (especially in PCM) can consume a lot of disk space, so your architecture should include a way to compress or discard raw audio after transcription.

Many solutions use a **hybrid approach**: they capture audio in real-time (so nothing is missed), but perform heavy tasks like summarization once the session is over. For instance, you might do a light real-time transcription for immediate features (like showing captions or detecting a wake word), but also save the raw audio to run a more accurate Whisper transcription afterward for the final archive/summarization.

**Feeding Audio to Transcription Engines** – Modern transcription models (like Whisper) and services typically expect uncompressed PCM audio or certain common formats:

* If you’re using an **API service** (AssemblyAI, Google, etc.), you might send audio via a REST endpoint or websocket. Most APIs accept WAV or FLAC files for batch processing. In streaming mode, you send raw PCM bytes or opus frames over their websocket protocol. In our case, Discord gives Opus frames. You’ll need to decode these to PCM unless the API accepts Opus. Often a library like `prism-media` (for Node) or `PyNaCl` (for Python, which binds libsodium/Opus) is used to decode Opus to raw audio frames. Ensure the audio is at the sample rate the STT expects. For example, AssemblyAI’s realtime SDK explicitly sets sampleRate to 48000 Hz to match Discord’s output.
* If you’re using **OpenAI Whisper** locally, you can feed it a WAV/MP3/FLAC file or a NumPy array of audio samples. Whisper will resample audio internally to 16 kHz mono, but you should still provide high-quality input. Generally, you’ll decode the Discord audio to a WAV (PCM 16-bit) for Whisper. It’s wise to preserve quality by not downmixing or downsampling too much before transcription – Whisper was trained mostly on 16 kHz, but giving it the full 48 kHz audio doesn’t hurt (it will downsample internally). What you *should* avoid is unnecessary lossy compression. For instance, do not re-encode the audio to a low-bitrate MP3 before transcription, as that could drop accuracy.

**Recommended Audio Format** – For best accuracy, record in a **lossless, high-fidelity format**. Discord’s native Opus (\~24kbps) is already a compressed format (designed for speech). Transcription models can handle Opus-compressed audio, but you don’t want to compress it further. A common practice is to decode Opus to raw PCM and either store as `.wav` (which is PCM in a RIFF container) or as an uncompressed `.pcm` file. The Discord recorder bot example writes files in PCM 16-bit little-endian, 48 kHz stereo – this retains the full quality that Discord delivered. You could alternatively compress to **FLAC** which is lossless (reduces file size significantly without losing quality – useful for archiving long sessions before transcription). **Opus in Ogg** is another option: since Discord already uses Opus, you can directly packetize those frames into an Ogg file (essentially what Craig bot does, offering FLAC or AAC downloads too). However, not all STT services accept Ogg Opus input, so you might need to decode it when transcribing. Overall, **16-bit PCM** at 48 kHz is a safe choice as it’s widely accepted by speech-to-text systems and avoids any fidelity loss.

## Audio Storage and Management

When designing the recording storage, consider how to organize files and keep the system maintainable:

* **File Splitting** – It’s simplest to maintain one audio file per user per session. This avoids mixing and allows easy per-speaker transcription. For example, if three users are in a channel, your bot can create three files (one for each user’s audio). Use a clear naming scheme that ties together the guild/channel, the user, and maybe the date/time or session ID. For instance, naming files like `Guild123_channel456_user789.opus` or including a timestamp (e.g. `voice12345_uid67890_2025-07-18_19-00-00.wav`) will help you identify them. Unique filenames prevent collisions and make it easier to automate processing (the name itself can carry metadata about whose audio it is).
* **Synchronization** – If you later need a combined audio (for example, to run a diarization algorithm that expects one file, or simply to have a reference of the whole conversation), you’ll have to mix the per-user tracks. This is tricky because of timing differences. In practice, you might insert silence into quieter tracks to align them. The Craig bot addresses this by ensuring all tracks are equal length and time-aligned (in Craig’s downloads, all files can be loaded into a DAW and they’ll line up perfectly). Achieving this yourself means tracking timestamp offsets for each packet or using the RTP sequence numbers to place audio in the correct timeline. Some libraries (like the discord.py VoiceRecv extension) note that Discord does **not** send absolute timestamps for synchronization, so you may need to estimate timing by packet arrival if creating a single mixed file. An easier workaround for transcription purposes is usually to **transcribe each track separately** (since diarization is trivial if you already know which user each track belongs to) and then merge the transcripts by time. If exact timestamps are needed for words, you could note when packets arrived relative to start.
* **Storage Format** – As mentioned, raw PCM can be large (one minute of 48 kHz stereo PCM is \~11 MB). Consider using FLAC or keeping the audio in the original Opus form to save space if you need to store hours of audio. If using Opus, you can either store the raw RTP packets (with some container) or repacketize into an Ogg Opus file. The Nostrum library example provided a method to assemble Opus packets of one SSRC into a playable Ogg Opus bitstream. This would let you store each user’s audio in a compressed form nearly identical to what was transmitted, with minimal overhead.
* **Cleanup and Retention** – It’s important to have a strategy for deleting or archiving recordings, both for privacy and storage management. For a transcription service, you might decide to delete the raw audio soon after producing the transcript and summary (unless you need it for future reference or to improve transcription accuracy later). The open-source recorder by Chebro, for instance, warns users to **empty the recordings folder after each session** once files are merged or no longer needed. Automated cleanup (e.g. removing files older than X days) can prevent your disk from filling up. Also consider user privacy – perhaps allow server admins to request deletion of recordings or not store audio longer than necessary.
* **File Upload/Access** – If your service will present transcripts or summaries to users, decide if the raw or processed audio needs to be accessible (for example, downloading the audio or listening to it). If not, you can keep the files in a secure backend. If yes, you might store final audio (or the combined track) in cloud storage and share a link. Be mindful of Discord’s privacy expectations here as well.

By following these practices – capturing each speaker separately, using clear file naming, preserving audio quality, and respecting consent and privacy – you can build a robust Discord recording architecture. In summary, the Discord API *does* allow high-fidelity multi-user recording with proper use of the voice gateway, and with the help of community libraries you can obtain individual audio streams for diarization. The key is to handle the data responsibly: both technically (ensuring synchronization and quality for accurate transcription) and legally (ensuring all users are aware and have agreed to being recorded). With that in place, you can feed the audio into transcription engines like Whisper (via either real-time streams or batch jobs) and produce speaker-labeled transcripts ready for summarization.

**Sources:**

* Discord Voice API documentation and community guides
* Open-source Discord recording bot implementations (e.g. Craig, discord.js recorder)
* Discord.js and discord.py library references for voice receive functionality
* Discord Terms of Service / Privacy discussions regarding recording
* AssemblyAI blog – *Building a Discord voice bot with transcription* (demonstrating real-time transcription pipeline)
* Additional notes from Discord community Q\&A and library docs on audio handling and best practices

